{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a71f680-b5d7-4c8d-9e77-d9f0e7534d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff95c231-aefc-48d1-9d71-c5fda1abc404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HR2O_NL(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, kernel_size=3, mlp_1x1=False):\n",
    "        super(HR2O_NL, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        padding = kernel_size // 2\n",
    "        self.conv_q = nn.Conv2d(hidden_dim, hidden_dim, kernel_size, padding=padding, bias=False)\n",
    "        self.conv_k = nn.Conv2d(hidden_dim, hidden_dim, kernel_size, padding=padding, bias=False)\n",
    "        self.conv_v = nn.Conv2d(hidden_dim, hidden_dim, kernel_size, padding=padding, bias=False)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.3)\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            hidden_dim, hidden_dim,\n",
    "            1 if mlp_1x1 else kernel_size,\n",
    "            padding=0 if mlp_1x1 else padding,\n",
    "            bias=False\n",
    "        )\n",
    "        self.norm = nn.GroupNorm(1, hidden_dim, affine=True)\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.conv_q(x).unsqueeze(1)\n",
    "        key = self.conv_k(x).unsqueeze(0)\n",
    "        att = (query * key).sum(2) / (self.hidden_dim ** 0.5)\n",
    "        att = nn.Softmax(dim=1)(att)\n",
    "        value = self.conv_v(x)\n",
    "        virt_feats = (att.unsqueeze(2) * value).sum(1)\n",
    "\n",
    "        virt_feats = self.norm(virt_feats)\n",
    "        virt_feats = self.lrelu(virt_feats)\n",
    "        virt_feats = self.conv(virt_feats)\n",
    "        virt_feats = self.dp(virt_feats)\n",
    "\n",
    "        x = x + virt_feats\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f066d9-b999-492b-bd75-009baa1647ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 512, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "hidden_dim = 512\n",
    "height = 32\n",
    "width = 32\n",
    "\n",
    "dummy_input = torch.randn(batch_size, hidden_dim, height, width)\n",
    "\n",
    "# Initialize the model\n",
    "model = HR2O_NL(hidden_dim=hidden_dim)\n",
    "\n",
    "# Pass the dummy data through the model\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Print the shape of the output\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e924cf1-b8aa-49f0-8dba-21d0d2473c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SceneUnderstandor(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, kernel_size=3, mlp_1x1=False):\n",
    "        super(SceneUnderstandor, self).__init__()\n",
    "\n",
    "        # The HR2O_NL layer (High Order Reasoning Operator)\n",
    "        self.hr2o_nl = HR2O_NL(hidden_dim=hidden_dim, kernel_size=kernel_size, mlp_1x1=mlp_1x1)\n",
    "\n",
    "        # Convolution layer to process RGB image (to get the same hidden_dim)\n",
    "        self.conv_rgb = nn.Conv2d(3, hidden_dim, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)\n",
    "\n",
    "        # Linear layers for ROI bounding boxes (8 points) and keypoints (84 points)\n",
    "        self.fc_roi = nn.Linear(8, hidden_dim)\n",
    "        self.fc_keypoints = nn.Linear(84, hidden_dim)\n",
    "        self.fc_keypoints\n",
    "        # A fully connected layer to integrate all features (RGB + ROI + keypoints)\n",
    "        self.fc_integrated = nn.Linear(3 * hidden_dim, hidden_dim)\n",
    "\n",
    "        # Additional layers for processing the integrated feature\n",
    "        self.fc_out = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, rgb_image, roi_bbox, keypoints):\n",
    "        # Step 1: Process the RGB image through convolution\n",
    "        rgb_features = self.conv_rgb(rgb_image)  # Shape: (batch_size, hidden_dim, height, width)\n",
    "        rgb_features = self.hr2o_nl(rgb_features)  # Apply the HR2O_NL layer\n",
    "\n",
    "        print(rgb_features.shape)\n",
    "        # Flatten the rgb_features tensor: (batch_size, hidden_dim, height, width) -> (batch_size, hidden_dim * height * width)\n",
    "        rgb_features = rgb_features.view(rgb_features.size(0), -1)\n",
    "\n",
    "        # Step 2: Process the ROI bounding box (8 points) and keypoints (84 points) through fully connected layers\n",
    "        roi_features = self.fc_roi(roi_bbox)  # Shape: (batch_size, hidden_dim)\n",
    "        keypoints_features = self.fc_keypoints(keypoints)  # Shape: (batch_size, hidden_dim)\n",
    "\n",
    "        print(rgb_features.shape, keypoints_features.shape, roi_features.shape)\n",
    "        # Step 3: Integrate the features (RGB, ROI, and keypoints)\n",
    "        integrated_features = torch.cat((rgb_features, roi_features, keypoints_features), dim=1)\n",
    "\n",
    "        print(integrated_features.shape)\n",
    "\n",
    "        # Step 4: Pass through fully connected layers to obtain the final output features\n",
    "        integrated_features = self.fc_integrated(integrated_features)\n",
    "        integrated_features = self.relu(integrated_features)\n",
    "\n",
    "        # Output layer (you can adjust the output size as needed)\n",
    "        output = self.fc_out(integrated_features)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eff8d845-ce24-42b4-ab9b-1edb9bbf8cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512, 32, 32])\n",
      "torch.Size([4, 524288]) torch.Size([4, 512]) torch.Size([4, 512])\n",
      "torch.Size([4, 525312])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x525312 and 1536x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m scene_understandor \u001b[38;5;241m=\u001b[39m SceneUnderstandor(hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Pass the dummy data through the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m scene_understandor(rgb_image, roi_bbox, keypoints)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Print the output shape\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[28], line 45\u001b[0m, in \u001b[0;36mSceneUnderstandor.forward\u001b[0;34m(self, rgb_image, roi_bbox, keypoints)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(integrated_features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Step 4: Pass through fully connected layers to obtain the final output features\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m integrated_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_integrated(integrated_features)\n\u001b[1;32m     46\u001b[0m integrated_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(integrated_features)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Output layer (you can adjust the output size as needed)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x525312 and 1536x512)"
     ]
    }
   ],
   "source": [
    "# Create dummy data\n",
    "batch_size = 4\n",
    "hidden_dim = 512\n",
    "height = 32\n",
    "width = 32\n",
    "\n",
    "# Dummy RGB image (batch_size, 3, height, width)\n",
    "rgb_image = torch.randn(batch_size, 3, height, width)\n",
    "\n",
    "# Dummy ROI bounding boxes (batch_size, 8) with two sets of 4 points each\n",
    "roi_bbox = torch.randn(batch_size, 8)\n",
    "\n",
    "# Dummy keypoints (batch_size, 84) representing 42 keypoints in x, y sequence\n",
    "keypoints = torch.randn(batch_size, 84)\n",
    "\n",
    "# Initialize the model\n",
    "scene_understandor = SceneUnderstandor(hidden_dim=hidden_dim)\n",
    "\n",
    "# Pass the dummy data through the model\n",
    "output = scene_understandor(rgb_image, roi_bbox, keypoints)\n",
    "\n",
    "# Print the output shape\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ddc33ed-d52d-4dea-9788-1d3c9e04a787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524288"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512*32*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01854276-d784-4c43-8ace-5ebb9e4ddce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f069bee-ef84-45b7-95ba-2494c0b98388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HR2O_NL(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, kernel_size=3, mlp_1x1=False):\n",
    "        super(HR2O_NL, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        padding = kernel_size // 2\n",
    "        self.conv_q = nn.Conv2d(hidden_dim, hidden_dim, kernel_size, padding=padding, bias=False)\n",
    "        self.conv_k = nn.Conv2d(hidden_dim, hidden_dim, kernel_size, padding=padding, bias=False)\n",
    "        self.conv_v = nn.Conv2d(hidden_dim, hidden_dim, kernel_size, padding=padding, bias=False)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.3)\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            hidden_dim, hidden_dim,\n",
    "            1 if mlp_1x1 else kernel_size,\n",
    "            padding=0 if mlp_1x1 else padding,\n",
    "            bias=False\n",
    "        )\n",
    "        self.norm = nn.GroupNorm(1, hidden_dim, affine=True)\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.conv_q(x).unsqueeze(1)\n",
    "        key = self.conv_k(x).unsqueeze(0)\n",
    "        att = (query * key).sum(2) / (self.hidden_dim ** 0.5)\n",
    "        att = nn.Softmax(dim=1)(att)\n",
    "        value = self.conv_v(x)\n",
    "        virt_feats = (att.unsqueeze(2) * value).sum(1)\n",
    "\n",
    "        virt_feats = self.norm(virt_feats)\n",
    "        virt_feats = self.lrelu(virt_feats)\n",
    "        virt_feats = self.conv(virt_feats)\n",
    "        virt_feats = self.dp(virt_feats)\n",
    "\n",
    "        x = x + virt_feats\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28b2e0e4-765e-48c2-b778-f62d582ffefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SceneUnderstandor(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, kernel_size=3, mlp_1x1=False):\n",
    "        super(SceneUnderstandor, self).__init__()\n",
    "\n",
    "        # High Order Reasoning Operator\n",
    "        self.hr2o_nl = HR2O_NL(hidden_dim=hidden_dim, kernel_size=kernel_size, mlp_1x1=mlp_1x1)\n",
    "\n",
    "        # Convolutional layer to process RGB image (to get the same hidden_dim)\n",
    "        self.conv_rgb = nn.Conv2d(3, hidden_dim, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)\n",
    "\n",
    "        # Linear layers for ROI bounding boxes (8 points) and keypoints (84 points)\n",
    "        self.fc_roi = nn.Linear(8, hidden_dim)  # ROI bbox to hidden_dim\n",
    "        self.fc_keypoints = nn.Linear(84, hidden_dim)  # Keypoints to hidden_dim\n",
    "\n",
    "        # A fully connected layer to integrate all features (RGB + ROI + keypoints)\n",
    "        self.fc_integrated = nn.Linear(3 * hidden_dim, hidden_dim)\n",
    "\n",
    "        # Additional layers for processing the integrated feature\n",
    "        self.fc_out = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def extract_roi_features(self, image, roi_bbox):\n",
    "        \"\"\"\n",
    "        Extract features from the image using the ROI bounding boxes.\n",
    "        The bbox is not normalized and represents raw pixel values.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = image.shape\n",
    "        # Assuming roi_bbox is of shape (batch_size, 8), representing a rectangular box (x1, y1, x2, y2, ...)\n",
    "        roi_features = []\n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            x1, y1, x2, y2 = roi_bbox[i].tolist()[:4]  # Get the first 4 points as the bounding box\n",
    "    \n",
    "            # Convert indices to integers to ensure proper slicing\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    \n",
    "            # Pooling over ROI (mean pooling over the ROI region)\n",
    "            roi_features.append(image[i, :, y1:y2, x1:x2].mean(dim=[1, 2]))  # Pooling over ROI\n",
    "    \n",
    "        roi_features = torch.stack(roi_features, dim=0)\n",
    "        return roi_features  # Shape: (batch_size, hidden_dim)\n",
    "\n",
    "\n",
    "    def extract_keypoints_features(self, image, keypoints):\n",
    "        \"\"\"\n",
    "        Extract features from the image using the keypoints.\n",
    "        The keypoints are not normalized.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = image.shape\n",
    "        keypoints_features = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Assuming keypoints is a flattened list of (x, y) coordinates\n",
    "            keypoints_list = keypoints[i].reshape(-1, 2).int()\n",
    "            keypoint_feature_map = torch.zeros_like(image[i, 0, :, :])  # Create a blank feature map for each keypoint\n",
    "\n",
    "            for (x, y) in keypoints_list:\n",
    "                if 0 <= x < width and 0 <= y < height:\n",
    "                    keypoint_feature_map[y, x] = 1  # Set pixel values at keypoint locations\n",
    "\n",
    "            keypoints_features.append(keypoint_feature_map.unsqueeze(0))  # Keep as a single channel feature map\n",
    "\n",
    "        keypoints_features = torch.stack(keypoints_features, dim=0)\n",
    "        return keypoints_features  # Shape: (batch_size, 1, height, width)\n",
    "\n",
    "    def forward(self, rgb_image, roi_bbox, keypoints):\n",
    "        # Step 1: Process the RGB image through convolution\n",
    "        rgb_features = self.conv_rgb(rgb_image)  # Shape: (batch_size, hidden_dim, height, width)\n",
    "\n",
    "        # Step 2: Extract ROI features from the image\n",
    "        roi_features = self.extract_roi_features(rgb_image, roi_bbox)  # Shape: (batch_size, hidden_dim)\n",
    "\n",
    "        # Step 3: Extract keypoints features from the image\n",
    "        keypoints_features = self.extract_keypoints_features(rgb_image, keypoints)  # Shape: (batch_size, 1, height, width)\n",
    "\n",
    "        # Step 4: Convert roi_features to the same size as the RGB image (height, width)\n",
    "        roi_features = roi_features.unsqueeze(2).unsqueeze(3).expand(-1, -1, rgb_features.size(2), rgb_features.size(3))\n",
    "\n",
    "        # Step 5: Concatenate the RGB image with the ROI and keypoints features\n",
    "        combined_features = torch.cat((rgb_features, roi_features, keypoints_features), dim=1)\n",
    "\n",
    "        # Step 6: Apply High Order Reasoning Operator (HR2O)\n",
    "        final_features = self.hr2o_nl(combined_features)\n",
    "\n",
    "        # Step 7: Flatten the features for fully connected layers\n",
    "        final_features = final_features.view(final_features.size(0), -1)\n",
    "\n",
    "        # Step 8: Pass through fully connected layers to obtain the final output features\n",
    "        integrated_features = self.fc_integrated(final_features)\n",
    "        integrated_features = self.relu(integrated_features)\n",
    "\n",
    "        # Output layer (you can adjust the output size as needed)\n",
    "        output = self.fc_out(integrated_features)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e555d43-9738-46a5-a3f3-e9463d256144",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [512, 512, 3, 3], expected input[4, 516, 32, 32] to have 512 channels, but got 516 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m scene_understandor \u001b[38;5;241m=\u001b[39m SceneUnderstandor(hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Pass the dummy data through the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m scene_understandor(rgb_image, roi_bbox, keypoints)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Print the output shape\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[39], line 83\u001b[0m, in \u001b[0;36mSceneUnderstandor.forward\u001b[0;34m(self, rgb_image, roi_bbox, keypoints)\u001b[0m\n\u001b[1;32m     80\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((rgb_features, roi_features, keypoints_features), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Step 6: Apply High Order Reasoning Operator (HR2O)\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m final_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhr2o_nl(combined_features)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Step 7: Flatten the features for fully connected layers\u001b[39;00m\n\u001b[1;32m     86\u001b[0m final_features \u001b[38;5;241m=\u001b[39m final_features\u001b[38;5;241m.\u001b[39mview(final_features\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[30], line 26\u001b[0m, in \u001b[0;36mHR2O_NL.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 26\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_q(x)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_k(x)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m     att \u001b[38;5;241m=\u001b[39m (query \u001b[38;5;241m*\u001b[39m key)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniforge3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [512, 512, 3, 3], expected input[4, 516, 32, 32] to have 512 channels, but got 516 channels instead"
     ]
    }
   ],
   "source": [
    "# Create dummy data\n",
    "batch_size = 4\n",
    "hidden_dim = 512\n",
    "height = 32\n",
    "width = 32\n",
    "\n",
    "# Dummy RGB image (batch_size, 3, height, width)\n",
    "rgb_image = torch.randn(batch_size, 3, height, width)\n",
    "\n",
    "# Dummy ROI bounding boxes (batch_size, 8) with two sets of 4 points each\n",
    "roi_bbox = torch.randn(batch_size, 8)\n",
    "\n",
    "# Dummy keypoints (batch_size, 84) representing 42 keypoints in x, y sequence\n",
    "keypoints = torch.randn(batch_size, 84)\n",
    "\n",
    "# Initialize the model\n",
    "scene_understandor = SceneUnderstandor(hidden_dim=hidden_dim)\n",
    "\n",
    "# Pass the dummy data through the model\n",
    "output = scene_understandor(rgb_image, roi_bbox, keypoints)\n",
    "\n",
    "# Print the output shape\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e9517-a2fd-4ae0-8049-864835e85130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c93eead-37cc-4686-8a83-be1dcd9ed21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan],\n",
       "        [nan, nan, nan],\n",
       "        [nan, nan, nan],\n",
       "        [nan, nan, nan]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_understandor.extract_roi_features(rgb_image, roi_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c233d11b-db12-4f19-85ab-ade0f744068f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
